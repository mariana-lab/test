# Week #1 Algorithms

Block: Block IIIII
Topic: algorithms
Type: üó∫¬† Materials

**Foreword**: Titles and bad jokes are temporary and here for the amusement of the author. Similarly any images used are placeholder and will be replaced in future with something (probably) better looking. 

# Algorithms: Not Just for Boffins

> ‚ÄúA step-by-step procedure for solving a problem or accomplishing some end‚Äù - Wikipedia, I guess.
> 

The introduction is always the hardest part, so I wrote some mental diarrhoea to make it okay for me to skip ahead and come back to this later. Ipsum lorem wingardium leviosa.

 

## Data Structures

Let us see the different ways we can organise and discuss data. When we talk about data it can be anything; a crowd of people, a stack of cards, a list of groceries, a family tree, a binder of Pok√©mon cards, a heap of clothes. 

The data would be the Pok√©mon cards, the groceries, the family itself, the clothes.

The data *structure* would be how these items are grouped together. It can be well organised like in the case of the family tree, the list, the stack or it can be unorganised such as the heap or the crowd.

Which one we use depends on the nature of the information itself. When I pop to the shops to buy food for dinner, I jot down everything I think I‚Äôll need - one after the other. There‚Äôs no particular order in which I plan to get the items and I can cross out the items as I go long.

![list.png](week1/list.png)

![queue.png](week1/queue.png)

When I go to the till to pay for the items, I have to stand in a queue. I get in behind the last person to arrive there and I‚Äôll only be seen once I‚Äôm at the front of the queue.

When I get home and unpack the bag of groceries, I take the first item from the top and work my way down. The first item I grab from the bag is the last item that went into it. 

![shopping bag.png](week1/shopping_bag.png)

As you can see, the key difference between these examples is how new data is added or removed - how information is accessed.

### Queue

![Untitled](week1/Untitled.png)

A queue is a structure that follows a FIFO access discipline. FIFO as in *First In, First Out*. Items are arranged in order of their arrival. The head of the queue contains the first element added and we can only ever access (and remove) whatever is at the head. 

### Stack

![Untitled](week1/Untitled%201.png)

A stack is a structure that follows a LIFO access discipline. LIFO as in *Last In, First Out*. The first item we can remove from the stack is the last item that was added to it. The element labelled ‚Äú1‚Äù will only be accessible once all the other elements are removed. Stacks can also be said to follow FILO as in *First In, Last Out*.

### Stacks and Queues

Both are abstract structures - meaning that they describe an *idea* of how elements should be added and removed. It is up to the programmer to decide how to best represent this idea using structures available to them in their preferred programming language. 

For instance, both stacks and queues can be implemented in JavaScript using arrays using a combination of methods like `push` ,  `pop` and `shift` . 

As an example, you‚Äôve unknowingly come into contact with the stack when using the *undo* function in any program and the queue is often implemented to simulate real life queues - such as when joining a multiplayer game server or reserving tickets online.

Stacks, queues and other structures are often used together to solve complex algorithms - it‚Äôs only one small part of a much bigger problem we are working on.

### Graph

![Visual Table - Frame 3 copy.jpg](week1/Visual_Table_-_Frame_3_copy.jpg)

Lists, stacks and queues are very *linear* in the manner that the elements are ordered.

A graph is a kind of non-linear data structure. Rather than being ordered by arrival, they are ordered by their relationship to each other. A graph consists of **nodes** (also known as vertices or V) and **edges** (the lines that connect nodes to each-other, E).

In the above example we can say that:

- V = {1, 2, 3, 4, 5}
- E = {(1, 2), (2,4), (4,5), (4,3), (3,5), (1,3)}

Where a new node is placed depends on its relationship to other nodes on the graph.

We can say that two nodes are **adjacent** to one another if an edge is connecting them.

Nodes 1 and 2 are adjacent, whilst 1 and 5 are not.

There is however a **path** between nodes 1 and 5; 1-3 and 3-5. A path is a sequence of edges that connect nodes.

Graphs can be **directed** or **undirected**. The above example is undirected, which means you can travel from the node 1-3 and 3-1 in both directions. An example would be if these nodes represented towns on a map, you can freely travel from Lisbon to Porto and vice versa.

In a directed graph, the edges have arrows. 1-3 would be possible, but 3-1 wouldn‚Äôt. An example of this would be followers on social media. Just because you follow someone on Instagram doesn‚Äôt mean they follow you back.

Here is how you would represent an undirected graph and which nodes are adjacent to one another, known as an **adjacency matrix**. A 0 means there are no edges connecting these nodes and a 1 means there are.

![Untitled](week1/Untitled%202.png)

Another way would be to represent the relationships as a list, using just the values of the nodes.

A ‚Üí B, C
B ‚Üí A, D
C ‚Üí A, D
D ‚Üí B, C

There‚Äôs no need to represent the lack of edges in this case, so it‚Äôs more space efficient. Imagine a graph with hundreds of nodes and how each of these examples might look. 

*What could we use to represent this adjacency list?*

### Tree

![Visual Table - Frame 4.jpg](week1/Visual_Table_-_Frame_4.jpg)

A tree is a special kind of graph. In the previous example, our graph had *cycles* - you could move from 3-4-5 and then back to 3. Or 1-2-4-3 and back to 1. In a tree, there are no cycles (this is called **acyclic).** The data is organised based on a hierarchy and each node can only have one parent. 

An example of this would be a family tree or your computer‚Äôs file system (think about how your computer organises the files - the folders inside folders)

### Graphs and Trees

Graphs and Trees are used to represent many real world problems. To be able to find the shortest path to a city (or to Pacman) it‚Äôs easier to represent the locations as a graph with paths leading to adjacent locations since a city can be connected to more than one other nearby city.

A graph can be used to represent your connections on a social media platform, to solve puzzles in games, to map popular search words to each-other, to determine the hierarchy of employees in a company (tree), as gaming dialogue trees, etc. Since they are non-linear they can represent the real world a lot easier than other structures.

Algorithms which need to travel across a graph or a tree will often use stacks or queues to help them - and the graphs themselves need to be implemented in something language specific. The visualisation is but one piece of the larger puzzle when problem solving.

## Big O Notation

Often times we are interested in finding the **best** algorithm to solve a particular problem. But what exactly makes one algorithm *better* than another? Mathematical notations can be used to describe the running time of a particular set of instructions by looking at how many **operations** it must perform.

When looking at a problem we can imagine either the worst-case scenario (Big O Notation), the best-case scenario (Omega Notation) or the average (Theta Notation). Since we don‚Äôt always have control over how data is organised, it‚Äôs best to look at the worst-case scenario when determining the *complexity* of an algorithm.

![lcokers.jpg](week1/lcokers.jpg)

Imagine standing in a hallway of lockers holding a key. The key opens one of these lockers, but you don‚Äôt know which one. If the hallway contains 100 lockers - the worst-case scenario would be the last locker you check being the right one. 100 lockers, 100 checks. That doesn‚Äôt mean you‚Äôll necessarily do 100 checks - imagine if you get lucky and get it right on the first try. 

It just means that it cannot be any worse than 100 checks. 

The mathematical notation for this would be O(n) where *n* is the number of elements. So the maximum amount of operations you might need to do is proportional to the number of elements.

![Visual Table - Frame 5.jpg](week1/Visual_Table_-_Frame_5.jpg)

**O(1) - constant**

An equation like `n * n + n` will always perform the same amount of operations, regardless of whether `n` is 5 or 5000. The **1** in **O(1)** is reductive. Even for an algorithm performing 100 operations (O(100)), if the number of operations is the same no matter the data we represent it as **O(1)**. In most programming languages, getting the size of an array is an algorithm  will also perform only one operation regardless of the number of elements.

 

**O(n) - linear**

Performing operations for each element is an example of **O(n)**, such as with simple loops. The amount of operations increases with the number of elements, just like in the lockers example.

**O(logn) - logarithmic**

A logarithm is the inverse of exponential - much like subtraction is the inverse of addition.

An exponent tells us how many times we need to multiply a number *by itself*. `2^3` means `2*2*2`, which is 8. Exponents will be covered more in-depth next week, so don‚Äôt worry if your math is a bit rusty.

A logarithm asks ‚Äúwhat exponent produced this number?‚Äù. It takes a base and divides a number by that until it reaches the number 1 and sees how many divisions were needed. 

So `log2(8)` where the `2` is the base would be:

8 / 2 = 4. 

4 / 2 = 2. 

2/2 = 1. 

The answer to `log2(8)` is 3. And `2^3` is 8. See the relationship?

Exponents are all about starting smaller and increasing, whereas logarithms start big and get smaller. They‚Äôre used to figure out how a certain result is achieved. Imagine you have a colony of ants which multiply in number quickly. You know that every hour the colony multiplies - so if you started with 1 and in an hour there would be 2, another hour 4 and so forth.

You could write this as `ants = 2^h` where `h` would be the number of hours. How many ants would we have in 3 hours? This is what an exponent answers. `2^3hours` which gives us 8!

But what if we only know the outcome? We know there are hundreds of ants and we want to know how long it took to get to that number. That‚Äôs where logarithms come to use.

A logarithmic algorithm is one where the number of operations is proportional to the logarithm of the number of elements. So for a data size of 8, 3 operations are needed. Often achieved with loops that continuously discard portions of a data set or through recursion.

**O(n^2) - quadratic**

Take the number of elements and square it and this gives you the worst-case scenario. This occurs when using nested loops.

```c
elements = 5;

for(number of elements)
	for(number of elements)
			print "hello"
```

In the above example, `hello` would be printed 25 times. Increase the number of elements to 6 and you get 6*6 times or 36.

**O(2^N) - exponential**

The number of operations doubles with each new element. Imagine an ice cream shop that allows for different combinations of toppings and wants you to find every possible combination. With just three toppings - sprinkles, gummies and chocolate - we get 8 possibilities (with none being one of them). 

![Visual Table - Frame 9.jpg](week1/Visual_Table_-_Frame_9.jpg)

Add a new topping, toffee. The number of combinations jumps to 16.

![Visual Table - Frame 10.jpg](week1/Visual_Table_-_Frame_10.jpg)

**O(n!) - factorial**

A factorial is the multiplication of all the whole numbers of a chosen number down to 1. We would write the factorial of 4 as `4!` and we would calculate it by multiplying `4 x 3 x 2 x 1`, which gives us 24. `5!` would be the previous sequence, then multiplied by 5 so `5 x 24 = 120`.

So an algorithm with time complexity of O(n!) means the number of operations is proportional to the factorial of the number of elements. Look to the *Traveling Salesman Problem* as an example of a problem which can be solved in O(n!).

Anything greater than the *linear* time complexity is considered bad, even terrible. So the obvious answer is to avoid using algorithms of these time complexities altogether, right? Unfortunately, it‚Äôs not that easy. Some problems simply cannot be solved efficiently. 

However, since there is almost *always* more than one solution possible it comes down to understanding how to measure their complexities and choose the best one based on where it falls on the graph.

 

## Search Algorithms

Whether you‚Äôre looking up content on the internet using keywords, searching your contact list for a particular number, finding a file somewhere on your computer, looking for the quickest route to a restaurant or trying to buy the cheapest ticket to the Bahamas - search algorithms are what make it possible. 

It‚Äôs what allows us to find a particular element within a data structure. The kind of search we do depends entirely on how that data is organised. Looking for a book in a library will go differently if the books are ordered alphabetically, aesthetically (by the colour of the cover) or not organised at all.

### Linear Search

![Visual Table - Frame 21.jpg](week1/Visual_Table_-_Frame_21.jpg)

This kind of search is a sequential algorithm, starting at one end and working through the list one by one until either the element has been found or we‚Äôve reached the end of the list. It has a time complexity of O(n).

```c
value = (desired element)

for each item in the list
	if item equals value, return the location.
```

**Optional Challenge**: Implement a linear search that searches the following data: `[17, 4, 13, 2, 8, 14, 18, 3, 99, 66, 32, 1]`. Create an algorithm that can search for *any* element we ask for.

When the element is found, print how many operations it took to find it.

### Binary Search

[when we know data is ordered!]

![Visual Table - Frame 22(2).jpg](week1/Visual_Table_-_Frame_22(2).jpg)

![Visual Table - Frame 23(3).jpg](week1/Visual_Table_-_Frame_23(3).jpg)

![Visual Table - Frame 24.jpg](week1/Visual_Table_-_Frame_24.jpg)

Used in sorted lists, the binary search works by repeatedly dividing a list in half, reducing the set of data needed to search. It has a time complexity of O(Log n).

```c
value = (desired element)

pick middle element
	if element is equal to value, return the location.
	if element is greater than value, middle becomes the end.
	if element is lesser than value, middle becomes the beginning.
repeat until either value is found or there are no more elements.
```

**Optional Challenge**: Implement a binary search that searches the following data: `[1, 2, 4, 6, 9, 10, 14, 15, 18, 22, 24, 27, 30, 37, 42]` Create an algorithm capable of searching for *any* element we ask for. When the element is found, print how many operations it took to find it.

### Depth First Search

DFS or Depth First Search is used to traverse graphs and trees recursively. This algorithm usually splits the data into two categories: visited and not visited. The idea is to mark each node so as not visit the same node more than once. It uses a stack structure to hold the nodes we wish to visit and a list to hold the visited nodes.

![Visual Table - Frame 26(1).jpg](week1/Visual_Table_-_Frame_26(1).jpg)

Node 0 is the first and is marked as visited. Then all of the adjacent nodes are put on the stack. After which we take the first element off the stack (in this case, 1) and visit it‚Äôs adjacent nodes. 

![Visual Table - Frame 27(1).jpg](week1/Visual_Table_-_Frame_27(1).jpg)

1 is marked as visited. Since 0 has already been marked visited and 1 has no more adjacent nodes which aren‚Äôt already on the stack, we move on to the next element from the stack - node 2.

![Visual Table - Frame 28.jpg](week1/Visual_Table_-_Frame_28.jpg)

Node 2 has an unvisited adjacent node - 4. So that goes onto the stack and is visited next.

![Untitled](week1/Untitled%203.png)

![Untitled](week1/Untitled%204.png)

The algorithm will repeat these steps: checking for unvisited nearby nodes, adding them to the stack then visiting one by one, until the stack is empty. It has a time complexity of O(V + E) where V = vertices (nodes) and E = edges. This way, all nodes are guaranteed to be checked, going in depth of each branch first.

**Optional** **Challenge**: What would the pseudocode for a DFS algorithm be?

### Breadth-First Search

BFS or Breadth First Search is an algorithm used to traverse graphs or trees recursively. Much like DFS, it splits the data into two categories: visited and not visited. It uses a queue structure to hold the nodes we wish to visit and a list of visited nodes.

![Untitled](week1/Untitled%205.png)

Node 0 is marked as visited and the adjacent unvisited nodes are put on the queue . Then a node is removed from the head of the queue and the algorithm continues.

![Untitled](week1/Untitled%206.png)

We visit the node from the head of the queue (1).  Since 1 doesn‚Äôt have any adjacent nodes that aren‚Äôt already in the queue - and since 0 has already been visited - we move on to the next node in the queue.

![Untitled](week1/Untitled%207.png)

Node 2 is marked as visited and the adjacent unvisited nodes are put on the queue. The next node in the queue is visited.

![Untitled](week1/Untitled%208.png)

![Untitled](week1/Untitled%209.png)

The algorithm will repeat these steps: checking for unvisited nearby nodes, adding them to the queue then visiting one by one, until the queue is empty. It has a time complexity of O(V + E) where V = vertices (nodes) and E = edges. This way, all nodes are guaranteed to be checked, by level of hierarchy.

**Optional** **Challenge**: What would the pseudocode for a BFS algorithm be?

### BFS vs. DFS

Did you notice how both of these graph traversal algorithms have very similar steps? The only change being the data structure they use to store where to go next. So what‚Äôs the difference and why would I choose one over the other?

Let‚Äôs take a look again at what happened when we visited node 2 in both examples.

**Depth First Search**

![Untitled](week1/Untitled%2010.png)

In **DFS** the next visited node was 3. Due to the nature of the stack structure, we travel to the *depths* of the graph until reaching a dead end before backtracking.

**Breadth First Search**

![Untitled](week1/Untitled%2011.png)

In **BFS** the next visited node was 4. Due to the nature of the queue structure, we travel across the *breadth* of the graph until reaching a dead end before backtracking.

See this again, but with trees.

**Depth First Search**

![Untitled](week1/Untitled%2012.png)

**Breadth First Search**

![Untitled](week1/Untitled%2013.png)

As you can see, **DFS** travels to the bottom of a path in the tree before fully traversing another path, whereas **BFS** fully traverses the width of the tree before moving down a level.

*Can you think of a scenario where it would make more sense to use DFS? What about BFS?*

*Imagine the decision tree for a puzzle game, where decisions lead to another. What kind of traversal makes sense to use there?*

## Sort Algorithms

How can we deal with large amounts of data more easily? Applying filters by lowest-to-highest price, viewing a games library alphabetically, browsing shows by genre on Netflix or listening to songs by popularity on Spotify - sorting algorithms organise vast amounts of data for easy retrieval.

### Bubble Sort

![Visual Table - Frame 11.jpg](week1/Visual_Table_-_Frame_11.jpg)

One of the simplest algorithms to implement. This works by moving one by one through a list, swapping the current element with the next if the current is greater (for ascending order). When reaching the end, the algorithm repeats again from the beginning until the set is completely rearranged. 

It uses a counter which increases every time an element is swapped for each pass-through. When there are no swaps on a pass-through, this means the algorithm can stop. It has a time complexity of 0(n^2).

```jsx
swapcount = 0

for each element
	if current element greater than next element, swap
			increase swapcount
end of cycle, if swapcount greater than 0, start again.
```

### Selection Sort

![Visual Table - Frame 12.jpg](week1/Visual_Table_-_Frame_12.jpg)

This algorithm works by repeatedly finding the smallest element in an unsorted list and placing it at the beginning. The ‚Äúbeginning‚Äù of the list increments after each pass.

The algorithm moves one by one through the list. 17 is marked as the smallest number until it is compared to its neighbour the number 6.

The smallest number is updated to be the number 6.

12 is larger than 6, so there is no update.

2 is smaller than 6, so the smallest number is updated to be 2.

5 is larger than 2, so there is no update.

![Visual Table - Frame 13.jpg](week1/Visual_Table_-_Frame_13.jpg)

Once each element has been compared, the smallest number is swapped with the first element and the algorithm starts again from the second element onward.

6 is marked as the smallest number.

12 is larger than 6, so there is no update.

17 is larger than 6, so there is no update.

5 is smaller than 6, so the smallest number is updated to be 5.

![Visual Table - Frame 14.jpg](week1/Visual_Table_-_Frame_14.jpg)

Once the algorithm‚Äôs starting point reaches the end of the list, the data has been sorted. It has a time complexity of O(n^2)

```c
set the first unsorted element as the smallest
for each of the unsorted elements
 if element less than smallest
	set element as the smallest
swap smallest with the first unsorted position
```

### Insertion Sort

This algorithm also divides the data into *sorted* and *unsorted*. It takes an element from the unsorted section comparing it to each element in the sorted section. It is then placed in the correct position based on a comparison with the adjacent elements.

![Visual Table - Frame 15.jpg](week1/Visual_Table_-_Frame_15.jpg)

The first element is assumed to be sorted, so the first element in the unsorted section would be the number 3.

3 is then compared with each element in the sorted section, in this case with just the number 7.

3 is smaller than 7, so it is placed at the beginning of the sorted section.

1 is now compared with each element in the sorted section. Since it is smaller than 3, it is placed at the beginning of the sorted section.

![Visual Table - Frame 16.jpg](week1/Visual_Table_-_Frame_16.jpg)

6 is now compared with each element of the sorted section.

6 is greater than 1, so the comparison continues.

6 is greater than 3, so the comparison continues.

6 is smaller than 7, so it is placed before 7 and after 3.

![Visual Table - Frame 17.jpg](week1/Visual_Table_-_Frame_17.jpg)

2 is now compared with each element in the sorted section. 

2 is greater than 1, so the comparison continues.

2 is smaller than 3, so it is placed before 3 and after 1.

When there are no more elements in the unsorted section, the algorithm finishes.

It has a worst-case time complexity of 0(n^2) and the best case of 0(n).

**Optional** **Challenge**: Can you think of what the worst-case and best-case scenario would be?

```jsx
mark first element as sorted
for each unsorted element
	compare with each sorted element
     if sorted element greater than current element; place before, stop.
		 if sorted element lesser than current element; continue
	if end of section, place element. // reframe this.
  
```

## Recursion vs. Iterative

![droste.jpg](week1/droste.jpg)

Ever heard of the Droste Effect? It‚Äôs the effect in art of a piece appearing *recursively* in itself, theoretically forever. In programming, recursion is a method of solving a problem by first solving smaller portions of the problem, using the same algorithm.

Imagine a long line of people at a venue that stretches all the way around the corner to the other side of the building. When you join the line you have no idea how many people there are in front of you (and you don‚Äôt want to leave the line to count them one by one or you might lose your place!)

![line.jpg](week1/line.jpg)

So you ask the person in front of you how many people are in front of them. They couldn‚Äôt possibly know themselves, so they ask the person in front of them, and so forth and so forth. 

Eventually, the question reaches the front of the line, when the person standing directly behind the first person in the queue asks them ‚ÄúHow many people are in front of you?‚Äù.

They answer ‚Äúzero‚Äù, because they‚Äôre at the front of the line.

Now, the person directly behind them can turn and say to the person behind *them* - ‚ÄúI have one person in front of me‚Äù.

*That* person can now say they have 2 people in front of them. And so forth and so forth.

Eventually, the answer reached us, the person we asked originally turns around and tells us ‚ÄúI have 1364 in front of me!‚Äù. At this point it might be a good idea to give up and do something else for the day.

By using the same question posed towards smaller and smaller data sets until we reach an eventual solution and then using the solutions together to solve the original problem - we have mastered the art of recursion.

Take the factorial. The multiplication of all the whole numbers of a chosen number down to 1.

`4!` or `4 * 3 * 2 * 1`.

```c
result = 1;

for(n = factorial; n > 1; n--) {
	result = result * n;
}	
```

First an iterative approach. Whenever we use a loop to solve a problem, we‚Äôre using iteration. 

Now, the same problem but solved in a recursive manner. First, we identify the sub-problems and the *base case* - the moment where we have a clear answer.

4! can be thought of as 4 * 3!.

3! as 3 * 2!

2! as 2 * 1!

1! is always 1. Here, we discover our base case. That‚Äôs when we‚Äôll know we‚Äôve reached the end of all our sub-problems. We also identify a pattern. To discover the answer to 4!, we must discover the answer to 3! and so forth. 

```c
factorial(n){
		if(n === 1) {   // our base case
				return 1;
		}

		return n * factorial(n - 1); // recursive use of the same algorithm
}
```

So our algorithm works by posing the same question to smaller and smaller numbers until we reach the number 1. Then, the answers are used to solve the previous questions. 

![Visual Table - Frame 19.jpg](week1/Visual_Table_-_Frame_19.jpg)

![Visual Table - Frame 20.jpg](week1/Visual_Table_-_Frame_20.jpg)

**Optional Challenge**

Consider the **Fibonacci Sequence**. 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 and so on.

Each number is the sum of the two previous numbers. 

0 + 1 = 1. 

1 + 1 = 2. 

2 + 1 = 3.

Can you come up with an algorithm to determine the number at a certain position of the Fibonacci sequence? Example: Find the 5th Fibonacci number (which would be 3).

Try both iterative and recursive versions. Remember to pseudocode first!

## Pseudocode: Putting ideas to paper.

The majority of the examples in this paper are done using *pseudocode*. It‚Äôs agnostic to any programming language and used to help map out the flow of a program, without getting bogged down by the details. Pseudocode is essential to any programmer.

Think about programming tasks you‚Äôve dealt with in the past. Has there ever been a moment where you stepped away from the computer to go out for a walk, have a beer with friends or get in the shower and ‚ÄúEureka!‚Äù the solution seems to pop into your head? Whether thinking out loud, drawing out a problem to better visualise it or writing down a step-by-step process in a notepad - really thinking about the problem before grabbing the keyboard is the first step to reaching a solution.

There are no hard rules to writing pseudocode. Imagine an algorithm for eating a bowl of soup.

You could write this down as a series of steps.

```c
1 - pick up bowl
2 - eat some soup
3 - still hungry and there's still soup in the bowl? repeat step 2.
```

There‚Äôs no trace of ‚Äúcode‚Äù here, just plain logic. Then you could revise this using some keywords and indentation.

```c
pick up bowl
while hungry and still have soup
  eat some soup
```

It looks closer to what you might write in your preferred programming language now.

If you want, you could even include some pseudo-variables.

```c
hungry = true
soupQuantity = 5

pick up bowl
while hungry and still have soup
	eat some soup
	soupQuantity--
```

This final style of pseudocode is best left for revising your ideas when you already have a clear vision of how to solve the problem. Something closer to the first or second is much preferred.